{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ababbd52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.sem import logic\n",
    "from nltk.sem import Expression\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1dd4089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in folio data\n",
    "import pandas as pd\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "splits = {'train': 'folio_v2_train.jsonl', 'validation': 'folio_v2_validation.jsonl'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "718dc2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_json(\"hf://datasets/yale-nlp/FOLIO/\" + splits[\"train\"], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6ce102c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       All people who regularly drink coffee are depe...\n",
      "1       All people who regularly drink coffee are depe...\n",
      "2       All people who regularly drink coffee are depe...\n",
      "3       All people who regularly drink coffee are depe...\n",
      "4       Miroslav Venhoda was a Czech choral conductor ...\n",
      "                              ...                        \n",
      "996     Any convicted criminal that is innocent is not...\n",
      "997     Any convicted criminal that is innocent is not...\n",
      "998     Any convicted criminal that is innocent is not...\n",
      "999     Phoneix's music is classified under the indie ...\n",
      "1000    Phoneix's music is classified under the indie ...\n",
      "Name: premises, Length: 1001, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train['premises'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "124c700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert premise FOL to nltk representation so that we can convert to expression to ultimately pass to prover\n",
    "\n",
    "######\n",
    "# function citation: \n",
    "# from https://github.com/benlipkin/linc/blob/main/eval/tasks/utils.py\n",
    "######\n",
    "def convert_to_nltk_rep(logic_formula):\n",
    "    translation_map = {\n",
    "        \"∀\": \"all \",\n",
    "        \"∃\": \"exists \",\n",
    "        \"→\": \"->\",\n",
    "        \"¬\": \"-\",\n",
    "        \"∧\": \"&\",\n",
    "        \"∨\": \"|\",\n",
    "        \"⟷\": \"<->\",\n",
    "        \"↔\": \"<->\",\n",
    "        \"0\": \"Zero\",\n",
    "        \"1\": \"One\",\n",
    "        \"2\": \"Two\",\n",
    "        \"3\": \"Three\",\n",
    "        \"4\": \"Four\",\n",
    "        \"5\": \"Five\",\n",
    "        \"6\": \"Six\",\n",
    "        \"7\": \"Seven\",\n",
    "        \"8\": \"Eight\",\n",
    "        \"9\": \"Nine\",\n",
    "        \".\": \"Dot\",\n",
    "        \"Ś\": \"S\",\n",
    "        \"ą\": \"a\",\n",
    "        \"’\": \"\",\n",
    "    }\n",
    "\n",
    "    constant_pattern = r'\\b([a-z]{2,})(?!\\()'\n",
    "    logic_formula = re.sub(constant_pattern, lambda match: match.group(1).capitalize(), logic_formula)\n",
    "\n",
    "    for key, value in translation_map.items():\n",
    "        logic_formula = logic_formula.replace(key, value)\n",
    "\n",
    "    quant_pattern = r\"(all\\s|exists\\s)([a-z])\"\n",
    "    def replace_quant(match):\n",
    "        return match.group(1) + match.group(2) + \".\"\n",
    "    logic_formula = re.sub(quant_pattern, replace_quant, logic_formula)\n",
    "\n",
    "    dotted_param_pattern = r\"([a-z])\\.(?=[a-z])\"\n",
    "    def replace_dotted_param(match):\n",
    "        return match.group(1)\n",
    "    logic_formula = re.sub(dotted_param_pattern, replace_dotted_param, logic_formula)\n",
    "\n",
    "    simple_xor_pattern = r\"(\\w+\\([^()]*\\)) ⊕ (\\w+\\([^()]*\\))\"\n",
    "    def replace_simple_xor(match):\n",
    "        return (\"((\" + match.group(1) + \" & -\" + match.group(2) + \") | (-\" + match.group(1) + \" & \" + match.group(2) + \"))\")\n",
    "    logic_formula = re.sub(simple_xor_pattern, replace_simple_xor, logic_formula)\n",
    "\n",
    "    complex_xor_pattern = r\"\\((.*?)\\)\\) ⊕ \\((.*?)\\)\\)\"\n",
    "    def replace_complex_xor(match):\n",
    "        return (\"(((\" + match.group(1) + \")) & -(\" + match.group(2) + \"))) | (-(\" + match.group(1) + \")) & (\" + match.group(2) + \"))))\")\n",
    "    logic_formula = re.sub(complex_xor_pattern, replace_complex_xor, logic_formula)\n",
    "\n",
    "    special_xor_pattern = r\"\\(\\(\\((.*?)\\)\\)\\) ⊕ (\\w+\\([^()]*\\))\"\n",
    "    def replace_special_xor(match):\n",
    "        return (\"(((\" + match.group(1) + \")) & -\" + match.group(2) + \") | (-(\" + match.group(1) + \")) & \" + match.group(2) + \")\")\n",
    "    logic_formula = re.sub(special_xor_pattern, replace_special_xor, logic_formula)\n",
    "    \n",
    "    return logic_formula\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "67ec79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['premises-FOL'] = [ convert_to_nltk_rep(p) for p in train['premises-FOL']]\n",
    "train['conclusion-FOL'] = train['conclusion-FOL'].apply(convert_to_nltk_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "286ae446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "n = 23\n",
    "n = 60\n",
    "n = 148\n",
    "n= 261\n",
    "\n",
    "# test\n",
    "#n = 850\n",
    "\n",
    "print(train['label'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "48086646",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every chef can cook.\n",
      "Some people who aren’t chefs can cook.\n",
      "People who cook can make scrambled eggs and pasta.\n",
      "If someone can make cookies and muffins, they are a baker.\n",
      "Bakers who can also make scrambled eggs can make a good breakfast.\n",
      "Luke can make cookies, scrambled eggs, and muffins, but not pasta.\n"
     ]
    }
   ],
   "source": [
    "print(train['premises'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fdb89939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luke is a chef.\n"
     ]
    }
   ],
   "source": [
    "print(train['conclusion'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a1895a22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all x. (Chef(x) -> Can(x, Cook))\n",
      "exists x. (-Chef(x) & Can(x, Cook))\n",
      "all x. (Can(x, Cook) -> (CanMake(x, ScrambledEggs) & CanMake(x, Pasta)))\n",
      "all x. (CanMake(x, Cookies) & CanMake(x, Muffins) -> Baker(x))\n",
      "all x. ((Baker(x) & CanMake(x, ScrambledEggs)) -> CanMake(x, GoodBreakfast))\n",
      "CanMake(Luke, Cookies) & (CanMake(Luke, ScrambledEggs) & CanMake(Luke, Muffins) & -CanMake(Luke, Pasta)\n"
     ]
    }
   ],
   "source": [
    "print(train['premises-FOL'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6e655909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chef(Luke)\n"
     ]
    }
   ],
   "source": [
    "print(train['conclusion-FOL'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "11e9f42f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: Every chef can cook.\n",
      "FOL: all x. (Chef(x) -> Can(x, Cook))\n",
      "\n",
      "TEXT: Some people who aren’t chefs can cook.\n",
      "FOL: exists x. (-Chef(x) & Can(x, Cook))\n",
      "\n",
      "TEXT: People who cook can make scrambled eggs and pasta.\n",
      "FOL: all x. (Can(x, Cook) -> (CanMake(x, ScrambledEggs) & CanMake(x, Pasta)))\n",
      "\n",
      "TEXT: If someone can make cookies and muffins, they are a baker.\n",
      "FOL: all x. (CanMake(x, Cookies) & CanMake(x, Muffins) -> Baker(x))\n",
      "\n",
      "TEXT: Bakers who can also make scrambled eggs can make a good breakfast.\n",
      "FOL: all x. ((Baker(x) & CanMake(x, ScrambledEggs)) -> CanMake(x, GoodBreakfast))\n",
      "\n",
      "TEXT: Luke can make cookies, scrambled eggs, and muffins, but not pasta.\n",
      "FOL: CanMake(Luke, Cookies) & (CanMake(Luke, ScrambledEggs) & CanMake(Luke, Muffins) & -CanMake(Luke, Pasta)\n",
      "\n",
      "TEXT: Luke is a chef.\n",
      "FOL: Chef(Luke)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p, f in zip(train['premises'][n].split('\\n'), train['premises-FOL'][n].split('\\n')):\n",
    "    print(f\"TEXT: {p.strip()}\\nFOL: {f.strip()}\\n\")\n",
    "print(f\"TEXT: {train['conclusion'][n].strip()}\\nFOL: {train['conclusion-FOL'][n].strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6c406c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build prompt\n",
    "\n",
    "normal = \"\"\"The following is a first-order logic (FOL) problem.\n",
    "The problem is to determine whether the conclusion follows from the premises.\n",
    "The premises are given in the form of a set of first-order logic sentences.\n",
    "The conclusion is given in the form of a single first-order logic sentence.\n",
    "The task is to translate each of the premises and conclusions into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\n",
    "Expressions should adhere to the format of the Python NLTK package logic module.\n",
    "\n",
    "Here are four examples of premises and corresponding FOL, preceded by whether the FOL evaluates to true or false.\n",
    "\n",
    "TEXT: A La Liga soccer team ranks higher than another La Liga soccer team if it receives more points.\n",
    "FOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & MorePoints(x, y) -> RankHigherThan(x, y))\n",
    "\n",
    "TEXT: If there are two La Liga soccer teams and neither has more points than the other, then the team which receives more points from the games between the two teams ranks higher.\n",
    "FOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & -MorePoints(x, y) & -MorePoints(y, x) & MorePointsInGameBetween(x, y) ->  RankHigherThan(x, y))\n",
    "\n",
    "TEXT: Real Madrid and Barcelona are both La Liga soccer teams.\n",
    "FOL: LaLigaSoccerTeam(RealMadrid) & LaLigaSoccerTeam(Barcelona)\n",
    "\n",
    "TEXT: Real Madrid received more points than Barcelona.\n",
    "FOL: MorePoints(RealMadrid, Barcelona)\n",
    "\n",
    "TEXT: Neither Real Madrid nor Barcelona received more points from the games between them.\n",
    "FOL: -MorePointsInGameBetween(RealMadrid, Barcelona) & -MorePointsInGameBetween(Barcelona, RealMadrid)\n",
    "\n",
    "TEXT: Real Madrid ranks higher than Barcelona.\n",
    "FOL: RankHigherThan(RealMadrid, Barcelona)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: All professional athletes spend most of their time on sports.\n",
    "FOL: all x. (ProfessionalAthlete(x) -> SpendOn(x, MostOfTheirTime, Sports))\n",
    "\n",
    "TEXT: All Olympic gold medal winners are professional athletes.\n",
    "FOL: all x. (OlympicGoldMedalWinner(x) -> ProfessionalAthlete(x))\n",
    "\n",
    "TEXT: No full-time scientists spend the majority of their time on sports.\n",
    "FOL: all x. (FullTimeScientist(x) -> -SpendOn(x, MostOfTheirTime, Sports))\n",
    "\n",
    "TEXT: All Nobel physics laureates are full-time scientists.\n",
    "FOL: all x. (NobelPhysicsLaureate(x) -> FullTimeScientist(x))\n",
    "\n",
    "TEXT: Amy spends the most time on sports, or Amy is an Olympic gold medal winner.\n",
    "FOL: SpendOn(Amy, MostOfTheirTime, Sports) | OlympicGoldMedalWinner(Amy)\n",
    "\n",
    "TEXT: If Amy is not a Nobel physics laureate, then Amy is not an Olympic gold medal winner.\n",
    "FOL: -NobelPhysicsLaureate(Amy) -> -OlympicGoldMedalWinner(Amy)\n",
    "\n",
    "TEXT: If Amy is not an Olympic gold medal winner, then Amy is a Nobel physics laureate.\n",
    "FOL: -OlympicGoldMedalWinner(Amy) -> NobelPhysicsLaureate(Amy)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: No songs are visuals.\n",
    "FOL: all x. (Song(x) -> -Visual(x))\n",
    "\n",
    "TEXT: All folk songs are songs.\n",
    "FOL: all x. (FolkSong(x) -> Song(x))\n",
    "\n",
    "TEXT: All videos are visuals.\n",
    "FOL: all x. (Video(x) -> Visual(x))\n",
    "\n",
    "TEXT: All movies are videos.\n",
    "FOL: all x. (Movie(x) -> Video(x))\n",
    "\n",
    "TEXT: All sci-fi movies are movies.\n",
    "FOL: all x. (ScifiMovie(x) -> Movie(x))\n",
    "\n",
    "TEXT: Inception is a sci-fi movie.\n",
    "FOL: ScifiMovie(Inception)\n",
    "\n",
    "TEXT: Mac is neither a folk song nor a sci-fi movie.\n",
    "FOL: -FolkSong(Mac) & -ScifiMovie(Mac)\n",
    "\n",
    "TEXT: Inception is a folk song.\n",
    "FOL: FolkSong(Inception)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: Every chef can cook.\n",
    "FOL: all x. (Chef(x) -> Can(x, Cook))\n",
    "\n",
    "TEXT: Some people who aren’t chefs can cook.\n",
    "FOL: exists x. (-Chef(x) & Can(x, Cook))\n",
    "\n",
    "TEXT: People who cook can make scrambled eggs and pasta.\n",
    "FOL: all x. (Can(x, Cook) -> (CanMake(x, ScrambledEggs) & CanMake(x, Pasta)))\n",
    "\n",
    "TEXT: If someone can make cookies and muffins, they are a baker.\n",
    "FOL: all x. (CanMake(x, Cookies) & CanMake(x, Muffins) -> Baker(x))\n",
    "\n",
    "TEXT: Bakers who can also make scrambled eggs can make a good breakfast.\n",
    "FOL: all x. ((Baker(x) & CanMake(x, ScrambledEggs)) -> CanMake(x, GoodBreakfast))\n",
    "\n",
    "TEXT: Luke can make cookies, scrambled eggs, and muffins, but not pasta.\n",
    "FOL: CanMake(Luke, Cookies) & (CanMake(Luke, ScrambledEggs) & CanMake(Luke, Muffins) & -CanMake(Luke, Pasta)\n",
    "\n",
    "TEXT: Luke is a chef.\n",
    "FOL: Chef(Luke)\n",
    "<EVALUATE>\n",
    "\n",
    "Translate the following premises and conclusions to FOL expressions that are parseable by NLTK.\n",
    "Only output the expressions.\n",
    "<PREMISES>\n",
    "Surprises are either fun or dreadful.\n",
    "All scares are surprises.\n",
    "</PREMISES>\n",
    "<CONCLUSION>\n",
    "All scares are fun.\n",
    "</CONCLUSION>\n",
    "<EVALUATE>\n",
    "\"\"\"\n",
    "\n",
    "bnf = \"\"\"The following is a first-order logic (FOL) problem.\n",
    "The problem is to determine whether the conclusion follows from the premises.\n",
    "The premises are given in the form of a set of first-order logic sentences.\n",
    "The conclusion is given in the form of a single first-order logic sentence.\n",
    "The task is to translate each of the premises and conclusions into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\n",
    "Expressions should adhere to the format of the Python NLTK package logic module.\n",
    "\n",
    "Follow the following Backus-Naur Form grammar to construct your expression.\n",
    "```\n",
    "<Expression> ::= <UniversalQuantifier> | <ExistentialQuantifier> | <Predicate> | <Not> | <And> | <Or> | <Implication> | <Biconditional> | <ParenthesizedExpression>\n",
    "\n",
    "\n",
    "<UniversalQuantifier> ::= \"all\" <Variable> . <Expression>\n",
    "\n",
    "\n",
    "<ExistentialQuantifier> ::= \"exists\" <Variable> . <Expression>\n",
    "\n",
    "<Variable> ::= \"x\" | \"y\" | \"z\" | <name>\n",
    "\n",
    "<Predicate> ::= <name> \"(\" <Terms> \")\"\n",
    "\n",
    "<Terms> ::= <Term> | <Term> \",\" <Terms>\n",
    "\n",
    "<Term> ::= <Variable> | <Constant>\n",
    "\n",
    "<Constant> ::= <name>\n",
    "\n",
    "<Not> ::= \"-\" <Expression>\n",
    "\n",
    "<And> ::= <Expression> \"&\" <Expression>\n",
    "\n",
    "<Or> ::= <Expression> \"|\" <Expression>\n",
    "\n",
    "<Implication> ::= <Expression> \"->\" <Expression>\n",
    "\n",
    "<Biconditional> ::= <Expression> \"<->\" <Expression>\n",
    "\n",
    "<ParenthesizedExpression> ::= \"(\" <Expression> \")\"\n",
    "\n",
    "<name> ::= <letter> <letters>\n",
    "\n",
    "<letter> ::= \"a\" | \"b\" | \"c\" | ... | \"z\" | \"A\" | \"B\" | \"C\" | ... | \"Z\"\n",
    "\n",
    "<letters> ::= <letter> | <letter> <letters>\n",
    "```\n",
    "\n",
    "Here are four examples of premises and corresponding FOL, preceded by whether the FOL evaluates to true or false.\n",
    "\n",
    "TEXT: A La Liga soccer team ranks higher than another La Liga soccer team if it receives more points.\n",
    "FOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & MorePoints(x, y) -> RankHigherThan(x, y))\n",
    "\n",
    "TEXT: If there are two La Liga soccer teams and neither has more points than the other, then the team which receives more points from the games between the two teams ranks higher.\n",
    "FOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & -MorePoints(x, y) & -MorePoints(y, x) & MorePointsInGameBetween(x, y) ->  RankHigherThan(x, y))\n",
    "\n",
    "TEXT: Real Madrid and Barcelona are both La Liga soccer teams.\n",
    "FOL: LaLigaSoccerTeam(RealMadrid) & LaLigaSoccerTeam(Barcelona)\n",
    "\n",
    "TEXT: Real Madrid received more points than Barcelona.\n",
    "FOL: MorePoints(RealMadrid, Barcelona)\n",
    "\n",
    "TEXT: Neither Real Madrid nor Barcelona received more points from the games between them.\n",
    "FOL: -MorePointsInGameBetween(RealMadrid, Barcelona) & -MorePointsInGameBetween(Barcelona, RealMadrid)\n",
    "\n",
    "TEXT: Real Madrid ranks higher than Barcelona.\n",
    "FOL: RankHigherThan(RealMadrid, Barcelona)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: All professional athletes spend most of their time on sports.\n",
    "FOL: all x. (ProfessionalAthlete(x) -> SpendOn(x, MostOfTheirTime, Sports))\n",
    "\n",
    "TEXT: All Olympic gold medal winners are professional athletes.\n",
    "FOL: all x. (OlympicGoldMedalWinner(x) -> ProfessionalAthlete(x))\n",
    "\n",
    "TEXT: No full-time scientists spend the majority of their time on sports.\n",
    "FOL: all x. (FullTimeScientist(x) -> -SpendOn(x, MostOfTheirTime, Sports))\n",
    "\n",
    "TEXT: All Nobel physics laureates are full-time scientists.\n",
    "FOL: all x. (NobelPhysicsLaureate(x) -> FullTimeScientist(x))\n",
    "\n",
    "TEXT: Amy spends the most time on sports, or Amy is an Olympic gold medal winner.\n",
    "FOL: SpendOn(Amy, MostOfTheirTime, Sports) | OlympicGoldMedalWinner(Amy)\n",
    "\n",
    "TEXT: If Amy is not a Nobel physics laureate, then Amy is not an Olympic gold medal winner.\n",
    "FOL: -NobelPhysicsLaureate(Amy) -> -OlympicGoldMedalWinner(Amy)\n",
    "\n",
    "TEXT: If Amy is not an Olympic gold medal winner, then Amy is a Nobel physics laureate.\n",
    "FOL: -OlympicGoldMedalWinner(Amy) -> NobelPhysicsLaureate(Amy)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: No songs are visuals.\n",
    "FOL: all x. (Song(x) -> -Visual(x))\n",
    "\n",
    "TEXT: All folk songs are songs.\n",
    "FOL: all x. (FolkSong(x) -> Song(x))\n",
    "\n",
    "TEXT: All videos are visuals.\n",
    "FOL: all x. (Video(x) -> Visual(x))\n",
    "\n",
    "TEXT: All movies are videos.\n",
    "FOL: all x. (Movie(x) -> Video(x))\n",
    "\n",
    "TEXT: All sci-fi movies are movies.\n",
    "FOL: all x. (ScifiMovie(x) -> Movie(x))\n",
    "\n",
    "TEXT: Inception is a sci-fi movie.\n",
    "FOL: ScifiMovie(Inception)\n",
    "\n",
    "TEXT: Mac is neither a folk song nor a sci-fi movie.\n",
    "FOL: -FolkSong(Mac) & -ScifiMovie(Mac)\n",
    "\n",
    "TEXT: Inception is a folk song.\n",
    "FOL: FolkSong(Inception)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: Every chef can cook.\n",
    "FOL: all x. (Chef(x) -> Can(x, Cook))\n",
    "\n",
    "TEXT: Some people who aren’t chefs can cook.\n",
    "FOL: exists x. (-Chef(x) & Can(x, Cook))\n",
    "\n",
    "TEXT: People who cook can make scrambled eggs and pasta.\n",
    "FOL: all x. (Can(x, Cook) -> (CanMake(x, ScrambledEggs) & CanMake(x, Pasta)))\n",
    "\n",
    "TEXT: If someone can make cookies and muffins, they are a baker.\n",
    "FOL: all x. (CanMake(x, Cookies) & CanMake(x, Muffins) -> Baker(x))\n",
    "\n",
    "TEXT: Bakers who can also make scrambled eggs can make a good breakfast.\n",
    "FOL: all x. ((Baker(x) & CanMake(x, ScrambledEggs)) -> CanMake(x, GoodBreakfast))\n",
    "\n",
    "TEXT: Luke can make cookies, scrambled eggs, and muffins, but not pasta.\n",
    "FOL: CanMake(Luke, Cookies) & (CanMake(Luke, ScrambledEggs) & CanMake(Luke, Muffins) & -CanMake(Luke, Pasta)\n",
    "\n",
    "TEXT: Luke is a chef.\n",
    "FOL: Chef(Luke)\n",
    "<EVALUATE>\n",
    "\n",
    "Translate the following premises and conclusions to FOL expressions that are parseable by NLTK.\n",
    "Only output the expressions.\n",
    "<PREMISES>\n",
    "Surprises are either fun or dreadful.\n",
    "All scares are surprises.\n",
    "</PREMISES>\n",
    "<CONCLUSION>\n",
    "All scares are fun.\n",
    "</CONCLUSION>\n",
    "<EVALUATE>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "61c199fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "api_key = \"AIzaSyBW5t2x2q24sqq_njm0-u98FIOYO7ZtkM4\"\n",
    "\n",
    "def call_api(key, prompt):\n",
    "    data = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    command = [\n",
    "        \"curl\",\n",
    "\n",
    "        \"-H\", \"Content-Type: application/json\",\n",
    "        \"-d\", json.dumps(data),\n",
    "        \"-X\", \"POST\", f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    result = subprocess.run(command, capture_output=True, text=True) \n",
    "    response_json = json.loads(result.stdout)\n",
    "    return response_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "acbe8c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "res = call_api(api_key, normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fa512a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidates': [{'content': {'parts': [{'text': 'all x. (Surprise(x) -> (Fun(x) | Dreadful(x)))\\nall x. (Scare(x) -> Surprise(x))\\nall x. (Scare(x) -> Fun(x))\\n'}],\n",
       "    'role': 'model'},\n",
       "   'finishReason': 'STOP',\n",
       "   'avgLogprobs': -0.00012995333721240362}],\n",
       " 'usageMetadata': {'promptTokenCount': 1254,\n",
       "  'candidatesTokenCount': 51,\n",
       "  'totalTokenCount': 1305},\n",
       " 'modelVersion': 'gemini-1.5-flash-latest'}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c9445466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from LINC\n",
    "\n",
    "def get_all_variables(text):\n",
    "    pattern = r'\\([^()]+\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    all_variables = []\n",
    "    for m in matches:\n",
    "        m = m[1:-1]\n",
    "        m = m.split(\",\")\n",
    "        all_variables += [i.strip() for i in m]\n",
    "    return list(set(all_variables))\n",
    "def reformat_fol(fol):\n",
    "    translation_map = {\n",
    "        \"0\": \"Zero\", \n",
    "        \"1\": \"One\",\n",
    "        \"2\": \"Two\",\n",
    "        \"3\": \"Three\",\n",
    "        \"4\": \"Four\",\n",
    "        \"5\": \"Five\",\n",
    "        \"6\": \"Six\",\n",
    "        \"7\": \"Seven\",\n",
    "        \"8\": \"Eight\",\n",
    "        \"9\": \"Nine\",\n",
    "        \".\": \"Dot\",\n",
    "        \"’\": \"\",\n",
    "        \"-\": \"_\",\n",
    "        \"'\": \"\",\n",
    "        \" \": \"_\"\n",
    "    }\n",
    "    all_variables = get_all_variables(fol)\n",
    "    for variable in all_variables:\n",
    "        variable_new = variable[:]\n",
    "        for k, v in translation_map.items():\n",
    "            variable_new = variable_new.replace(k, v)\n",
    "        fol = fol.replace(variable, variable_new)\n",
    "    return fol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "70146f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All xDot (Surprise(x) -> (Fun(x) | Dreadful(x)))',\n",
       " 'All xDot (Scare(x) -> Surprise(x))',\n",
       " 'All xDot (Scare(x) -> Fun(x))',\n",
       " '']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = res['candidates'][0]['content']['parts'][0]['text'].split('\\n')\n",
    "lines = [reformat_fol(l) for l in lines]\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7d8d58b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AllExpression all x.(Scare(x) -> Fun(x))>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "04a098f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LogicalExpressionException",
     "evalue": "Unexpected token: 'xDot'.\nAll xDot (Surprise(x) -> (Fun(x) | Dreadful(x)))\n    ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedTokenException\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/nltk/sem/logic.py:156\u001b[0m, in \u001b[0;36mLogicParser.parse\u001b[0;34m(self, data, signature)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minRange(\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedTokenException(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currentIndex \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LogicalExpressionException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mUnexpectedTokenException\u001b[0m: Unexpected token: 'xDot'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLogicalExpressionException\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Expression\u001b[38;5;241m.\u001b[39mfromstring(lines[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/nltk/sem/logic.py:957\u001b[0m, in \u001b[0;36mExpression.fromstring\u001b[0;34m(cls, s, type_check, signature)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_type_checking_logic_parser\u001b[38;5;241m.\u001b[39mparse(s, signature)\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_logic_parser\u001b[38;5;241m.\u001b[39mparse(s, signature)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/nltk/sem/logic.py:159\u001b[0m, in \u001b[0;36mLogicParser.parse\u001b[0;34m(self, data, signature)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LogicalExpressionException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e, data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m mapping[e\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LogicalExpressionException(\u001b[38;5;28;01mNone\u001b[39;00m, msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_check:\n\u001b[1;32m    162\u001b[0m     result\u001b[38;5;241m.\u001b[39mtypecheck(signature)\n",
      "\u001b[0;31mLogicalExpressionException\u001b[0m: Unexpected token: 'xDot'.\nAll xDot (Surprise(x) -> (Fun(x) | Dreadful(x)))\n    ^"
     ]
    }
   ],
   "source": [
    "Expression.fromstring(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfab5208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'All xDot (Surprise(x) -> (Fun(x) | Dreadful(x)))\\nAll xDot (Scare(x) -> Surprise(x))\\nAll xDot (Scare(x) -> Fun(x))\\n'\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"'All xDot (Surprise(x) -> (Fun(x) | Dreadful(x)))\\nAll xDot (Scare(x) -> Surprise(x))\\nAll xDot (Scare(x) -> Fun(x))\\n'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b86c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/santacoder\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/santacoder\", trust_remote_code=True)\n",
    "\n",
    "inputs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7f7191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPT2CustomModel' object has no attribute '_attn_implementation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# using normal prompt\u001b[39;00m\n\u001b[1;32m     11\u001b[0m inputs_n \u001b[38;5;241m=\u001b[39m tokenizer(normal, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m outputs_n \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs_n, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m) \n\u001b[1;32m     14\u001b[0m output_final_n \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs_n[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_final_n)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2025\u001b[0m         input_ids,\n\u001b[1;32m   2026\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2027\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mprepared_logits_warper,\n\u001b[1;32m   2028\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2029\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2030\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2031\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2032\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2033\u001b[0m     )\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1315\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1315\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m   1316\u001b[0m     input_ids,\n\u001b[1;32m   1317\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1318\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1319\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1320\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1321\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1322\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1323\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1324\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1325\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1326\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1327\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1328\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1329\u001b[0m )\n\u001b[1;32m   1330\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1032\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1029\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m# Attention mask.\u001b[39;00m\n\u001b[0;32m-> 1032\u001b[0m _use_sdpa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2CustomModel' object has no attribute '_attn_implementation'"
     ]
    }
   ],
   "source": [
    "# prompt the model\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/santacoder\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/santacoder\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# using normal prompt\n",
    "inputs_n = tokenizer(normal, return_tensors=\"pt\",truncation=True)\n",
    "outputs_n = model.generate(**inputs_n, max_new_tokens=200, do_sample=True, temperature=0.7) \n",
    "\n",
    "output_final_n = tokenizer.decode(outputs_n[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_final_n)\n",
    "\n",
    "\n",
    "# using bnf prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fef57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6741ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f851afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for prelim results 1: convert to  nltk expression + apply grammar (check for correctness of expression l8r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af142936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
