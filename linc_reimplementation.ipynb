{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2591e281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: nltk in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pandas in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from pandas) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: transformers in /shared/25rj4/.local/lib/python3.12/site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /shared/25rj4/.local/lib/python3.12/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from transformers) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /shared/25rj4/.local/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /shared/25rj4/.local/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyyaml\n",
      "Successfully installed pyyaml-6.0.2\n",
      "Requirement already satisfied: requests in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement time (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for time\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: torch in /shared/25rj4/.local/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /shared/25rj4/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install re\n",
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install transformers\n",
    "!pip install requests\n",
    "!pip install json\n",
    "!pip install time\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ababbd52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/25rj4/.conda/envs/cs375/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.sem import logic\n",
    "from nltk.sem import Expression\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9bd9099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/shared/25rj4/.local/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/shared/25rj4/.local/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n",
      "    service.run()\n",
      "  File \"/shared/25rj4/.local/lib/python3.12/site-packages/huggingface_hub/commands/user.py\", line 153, in run\n",
      "    login(\n",
      "  File \"/shared/25rj4/.local/lib/python3.12/site-packages/huggingface_hub/_login.py\", line 123, in login\n",
      "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
      "  File \"/shared/25rj4/.local/lib/python3.12/site-packages/huggingface_hub/_login.py\", line 275, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/shared/bin/anaconda3/lib/python3.12/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/shared/bin/anaconda3/lib/python3.12/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen codecs>\", line 319, in decode\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd4089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in folio data\n",
    "# login using e.g. `huggingface-cli login` to access this dataset\n",
    "splits = {'train': 'folio_v2_train.jsonl', 'validation': 'folio_v2_validation.jsonl'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "718dc2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       All people who regularly drink coffee are depe...\n",
      "1       All people who regularly drink coffee are depe...\n",
      "2       All people who regularly drink coffee are depe...\n",
      "3       All people who regularly drink coffee are depe...\n",
      "4       Miroslav Venhoda was a Czech choral conductor ...\n",
      "                              ...                        \n",
      "996     Any convicted criminal that is innocent is not...\n",
      "997     Any convicted criminal that is innocent is not...\n",
      "998     Any convicted criminal that is innocent is not...\n",
      "999     Phoneix's music is classified under the indie ...\n",
      "1000    Phoneix's music is classified under the indie ...\n",
      "Name: premises, Length: 1001, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## DATA EXPLORATION\n",
    "\n",
    "train = pd.read_json(\"hf://datasets/yale-nlp/FOLIO/\" + splits[\"train\"], lines=True)\n",
    "print(train['premises'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "124c700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert premise-FOL to nltk representation so that we can convert to Expression (ultimately to pass to the prover)\n",
    "\n",
    "### \n",
    "######\n",
    "# function citation: \n",
    "# from https://github.com/benlipkin/linc/blob/main/eval/tasks/utils.py\n",
    "######\n",
    "def convert_to_nltk_rep(logic_formula):\n",
    "    translation_map = {\n",
    "        \"∀\": \"all \",\n",
    "        \"∃\": \"exists \",\n",
    "        \"→\": \"->\",\n",
    "        \"¬\": \"-\",\n",
    "        \"∧\": \"&\",\n",
    "        \"∨\": \"|\",\n",
    "        \"⟷\": \"<->\",\n",
    "        \"↔\": \"<->\",\n",
    "        \"0\": \"Zero\",\n",
    "        \"1\": \"One\",\n",
    "        \"2\": \"Two\",\n",
    "        \"3\": \"Three\",\n",
    "        \"4\": \"Four\",\n",
    "        \"5\": \"Five\",\n",
    "        \"6\": \"Six\",\n",
    "        \"7\": \"Seven\",\n",
    "        \"8\": \"Eight\",\n",
    "        \"9\": \"Nine\",\n",
    "        \".\": \"Dot\",\n",
    "        \"Ś\": \"S\",\n",
    "        \"ą\": \"a\",\n",
    "        \"’\": \"\",\n",
    "    }\n",
    "\n",
    "    constant_pattern = r'\\b([a-z]{2,})(?!\\()'\n",
    "    logic_formula = re.sub(constant_pattern, lambda match: match.group(1).capitalize(), logic_formula)\n",
    "\n",
    "    for key, value in translation_map.items():\n",
    "        logic_formula = logic_formula.replace(key, value)\n",
    "\n",
    "    quant_pattern = r\"(all\\s|exists\\s)([a-z])\"\n",
    "    def replace_quant(match):\n",
    "        return match.group(1) + match.group(2) + \".\"\n",
    "    logic_formula = re.sub(quant_pattern, replace_quant, logic_formula)\n",
    "\n",
    "    dotted_param_pattern = r\"([a-z])\\.(?=[a-z])\"\n",
    "    def replace_dotted_param(match):\n",
    "        return match.group(1)\n",
    "    logic_formula = re.sub(dotted_param_pattern, replace_dotted_param, logic_formula)\n",
    "\n",
    "    simple_xor_pattern = r\"(\\w+\\([^()]*\\)) ⊕ (\\w+\\([^()]*\\))\"\n",
    "    def replace_simple_xor(match):\n",
    "        return (\"((\" + match.group(1) + \" & -\" + match.group(2) + \") | (-\" + match.group(1) + \" & \" + match.group(2) + \"))\")\n",
    "    logic_formula = re.sub(simple_xor_pattern, replace_simple_xor, logic_formula)\n",
    "\n",
    "    complex_xor_pattern = r\"\\((.*?)\\)\\) ⊕ \\((.*?)\\)\\)\"\n",
    "    def replace_complex_xor(match):\n",
    "        return (\"(((\" + match.group(1) + \")) & -(\" + match.group(2) + \"))) | (-(\" + match.group(1) + \")) & (\" + match.group(2) + \"))))\")\n",
    "    logic_formula = re.sub(complex_xor_pattern, replace_complex_xor, logic_formula)\n",
    "\n",
    "    special_xor_pattern = r\"\\(\\(\\((.*?)\\)\\)\\) ⊕ (\\w+\\([^()]*\\))\"\n",
    "    def replace_special_xor(match):\n",
    "        return (\"(((\" + match.group(1) + \")) & -\" + match.group(2) + \") | (-(\" + match.group(1) + \")) & \" + match.group(2) + \")\")\n",
    "    logic_formula = re.sub(special_xor_pattern, replace_special_xor, logic_formula)\n",
    "    \n",
    "    return logic_formula\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67ec79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['premises-FOL'] = [ convert_to_nltk_rep(p) for p in train['premises-FOL']]\n",
    "train['conclusion-FOL'] = train['conclusion-FOL'].apply(convert_to_nltk_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "286ae446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "n = 23\n",
    "n = 60\n",
    "n = 148\n",
    "n= 261\n",
    "\n",
    "# test\n",
    "#n = 850\n",
    "\n",
    "print(train['label'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48086646",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every chef can cook.\n",
      "Some people who aren’t chefs can cook.\n",
      "People who cook can make scrambled eggs and pasta.\n",
      "If someone can make cookies and muffins, they are a baker.\n",
      "Bakers who can also make scrambled eggs can make a good breakfast.\n",
      "Luke can make cookies, scrambled eggs, and muffins, but not pasta.\n"
     ]
    }
   ],
   "source": [
    "print(train['premises'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdb89939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luke is a chef.\n"
     ]
    }
   ],
   "source": [
    "print(train['conclusion'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1895a22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all x. (Chef(x) -> Can(x, Cook))\n",
      "exists x. (-Chef(x) & Can(x, Cook))\n",
      "all x. (Can(x, Cook) -> (CanMake(x, ScrambledEggs) & CanMake(x, Pasta)))\n",
      "all x. (CanMake(x, Cookies) & CanMake(x, Muffins) -> Baker(x))\n",
      "all x. ((Baker(x) & CanMake(x, ScrambledEggs)) -> CanMake(x, GoodBreakfast))\n",
      "CanMake(Luke, Cookies) & (CanMake(Luke, ScrambledEggs) & CanMake(Luke, Muffins) & -CanMake(Luke, Pasta)\n"
     ]
    }
   ],
   "source": [
    "print(train['premises-FOL'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e655909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chef(Luke)\n"
     ]
    }
   ],
   "source": [
    "print(train['conclusion-FOL'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11e9f42f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: Every chef can cook.\n",
      "FOL: all x. (Chef(x) -> Can(x, Cook))\n",
      "\n",
      "TEXT: Some people who aren’t chefs can cook.\n",
      "FOL: exists x. (-Chef(x) & Can(x, Cook))\n",
      "\n",
      "TEXT: People who cook can make scrambled eggs and pasta.\n",
      "FOL: all x. (Can(x, Cook) -> (CanMake(x, ScrambledEggs) & CanMake(x, Pasta)))\n",
      "\n",
      "TEXT: If someone can make cookies and muffins, they are a baker.\n",
      "FOL: all x. (CanMake(x, Cookies) & CanMake(x, Muffins) -> Baker(x))\n",
      "\n",
      "TEXT: Bakers who can also make scrambled eggs can make a good breakfast.\n",
      "FOL: all x. ((Baker(x) & CanMake(x, ScrambledEggs)) -> CanMake(x, GoodBreakfast))\n",
      "\n",
      "TEXT: Luke can make cookies, scrambled eggs, and muffins, but not pasta.\n",
      "FOL: CanMake(Luke, Cookies) & (CanMake(Luke, ScrambledEggs) & CanMake(Luke, Muffins) & -CanMake(Luke, Pasta)\n",
      "\n",
      "TEXT: Luke is a chef.\n",
      "FOL: Chef(Luke)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p, f in zip(train['premises'][n].split('\\n'), train['premises-FOL'][n].split('\\n')):\n",
    "    print(f\"TEXT: {p.strip()}\\nFOL: {f.strip()}\\n\")\n",
    "print(f\"TEXT: {train['conclusion'][n].strip()}\\nFOL: {train['conclusion-FOL'][n].strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c406c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build prompt\n",
    "\n",
    "normalFragment = \"\"\"The following is a first-order logic (FOL) problem.\n",
    "The problem is to determine whether the conclusion follows from the premises.\n",
    "The premises are given in the form of a set of first-order logic sentences.\n",
    "The conclusion is given in the form of a single first-order logic sentence.\n",
    "The task is to translate each of the premises and conclusions into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\n",
    "Expressions should adhere to the format of the Python NLTK package logic module.\n",
    "\n",
    "Here are four examples of premises and corresponding FOL.\n",
    "\n",
    "TEXT: A La Liga soccer team ranks higher than another La Liga soccer team if it receives more points.\n",
    "FOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & MorePoints(x, y) -> RankHigherThan(x, y))\n",
    "\n",
    "TEXT: If there are two La Liga soccer teams and neither has more points than the other, then the team which receives more points from the games between the two teams ranks higher.\n",
    "FOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & -MorePoints(x, y) & -MorePoints(y, x) & MorePointsInGameBetween(x, y) ->  RankHigherThan(x, y))\n",
    "\n",
    "TEXT: Real Madrid and Barcelona are both La Liga soccer teams.\n",
    "FOL: LaLigaSoccerTeam(RealMadrid) & LaLigaSoccerTeam(Barcelona)\n",
    "\n",
    "TEXT: Real Madrid received more points than Barcelona.\n",
    "FOL: MorePoints(RealMadrid, Barcelona)\n",
    "\n",
    "TEXT: Neither Real Madrid nor Barcelona received more points from the games between them.\n",
    "FOL: -MorePointsInGameBetween(RealMadrid, Barcelona) & -MorePointsInGameBetween(Barcelona, RealMadrid)\n",
    "\n",
    "TEXT: Real Madrid ranks higher than Barcelona.\n",
    "FOL: RankHigherThan(RealMadrid, Barcelona)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: All professional athletes spend most of their time on sports.\n",
    "FOL: all x. (ProfessionalAthlete(x) -> SpendOn(x, MostOfTheirTime, Sports))\n",
    "\n",
    "TEXT: All Olympic gold medal winners are professional athletes.\n",
    "FOL: all x. (OlympicGoldMedalWinner(x) -> ProfessionalAthlete(x))\n",
    "\n",
    "TEXT: No full-time scientists spend the majority of their time on sports.\n",
    "FOL: all x. (FullTimeScientist(x) -> -SpendOn(x, MostOfTheirTime, Sports))\n",
    "\n",
    "TEXT: All Nobel physics laureates are full-time scientists.\n",
    "FOL: all x. (NobelPhysicsLaureate(x) -> FullTimeScientist(x))\n",
    "\n",
    "TEXT: Amy spends the most time on sports, or Amy is an Olympic gold medal winner.\n",
    "FOL: SpendOn(Amy, MostOfTheirTime, Sports) | OlympicGoldMedalWinner(Amy)\n",
    "\n",
    "TEXT: If Amy is not a Nobel physics laureate, then Amy is not an Olympic gold medal winner.\n",
    "FOL: -NobelPhysicsLaureate(Amy) -> -OlympicGoldMedalWinner(Amy)\n",
    "\n",
    "TEXT: If Amy is not an Olympic gold medal winner, then Amy is a Nobel physics laureate.\n",
    "FOL: -OlympicGoldMedalWinner(Amy) -> NobelPhysicsLaureate(Amy)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: No songs are visuals.\n",
    "FOL: all x. (Song(x) -> -Visual(x))\n",
    "\n",
    "TEXT: All folk songs are songs.\n",
    "FOL: all x. (FolkSong(x) -> Song(x))\n",
    "\n",
    "TEXT: All videos are visuals.\n",
    "FOL: all x. (Video(x) -> Visual(x))\n",
    "\n",
    "TEXT: All movies are videos.\n",
    "FOL: all x. (Movie(x) -> Video(x))\n",
    "\n",
    "TEXT: All sci-fi movies are movies.\n",
    "FOL: all x. (ScifiMovie(x) -> Movie(x))\n",
    "\n",
    "TEXT: Inception is a sci-fi movie.\n",
    "FOL: ScifiMovie(Inception)\n",
    "\n",
    "TEXT: Mac is neither a folk song nor a sci-fi movie.\n",
    "FOL: -FolkSong(Mac) & -ScifiMovie(Mac)\n",
    "\n",
    "TEXT: Inception is a folk song.\n",
    "FOL: FolkSong(Inception)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: Every chef can cook.\n",
    "FOL: all x. (Chef(x) -> Can(x, Cook))\n",
    "\n",
    "TEXT: Some people who aren’t chefs can cook.\n",
    "FOL: exists x. (-Chef(x) & Can(x, Cook))\n",
    "\n",
    "TEXT: People who cook can make scrambled eggs and pasta.\n",
    "FOL: all x. (Can(x, Cook) -> (CanMake(x, ScrambledEggs) & CanMake(x, Pasta)))\n",
    "\n",
    "TEXT: If someone can make cookies and muffins, they are a baker.\n",
    "FOL: all x. (CanMake(x, Cookies) & CanMake(x, Muffins) -> Baker(x))\n",
    "\n",
    "TEXT: Bakers who can also make scrambled eggs can make a good breakfast.\n",
    "FOL: all x. ((Baker(x) & CanMake(x, ScrambledEggs)) -> CanMake(x, GoodBreakfast))\n",
    "\n",
    "TEXT: Luke can make cookies, scrambled eggs, and muffins, but not pasta.\n",
    "FOL: CanMake(Luke, Cookies) & (CanMake(Luke, ScrambledEggs) & CanMake(Luke, Muffins) & -CanMake(Luke, Pasta)\n",
    "\n",
    "TEXT: Luke is a chef.\n",
    "FOL: Chef(Luke)\n",
    "<EVALUATE>\n",
    "\n",
    "Translate the following premises and conclusions to FOL expressions that are parseable by NLTK.\n",
    "Only output the expressions.\n",
    "<PREMISES>\n",
    "\"\"\"\n",
    "\n",
    "bnfFragment = \"\"\"The following is a first-order logic (FOL) problem.\n",
    "The problem is to determine whether the conclusion follows from the premises.\n",
    "The premises are given in the form of a set of first-order logic sentences.\n",
    "The conclusion is given in the form of a single first-order logic sentence.\n",
    "The task is to translate each of the premises and conclusions into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\n",
    "Expressions should adhere to the format of the Python NLTK package logic module.\n",
    "\n",
    "Follow the following Backus-Naur Form grammar to construct your expression. Example usages precede each rule in comments.\n",
    "```\n",
    "// Example: all x. (ProfessionalAthlete(x) -> SpendOn(x, MostOfTheirTime, Sports))\n",
    "<Expression> ::= <UniversalQuantifier> | <ExistentialQuantifier> | <Predicate> | <Not> | <And> | <Or> | <Implication> | <Biconditional> | <ParenthesizedExpression>\n",
    "\n",
    "// Example: all x. (Chef(x) -> Can(x, Cook))\n",
    "<UniversalQuantifier> ::= \"all\" <Variable> . <Expression>\n",
    "\n",
    "// Example: exists x. (-Chef(x) & Can(x, Cook))\n",
    "<ExistentialQuantifier> ::= \"exists\" <Variable> . <Expression>\n",
    "\n",
    "// Example: x\n",
    "<Variable> ::= \"x\" | \"y\" | \"z\" | <name>\n",
    "\n",
    "// Example: LaLigaSoccerTeam(RealMadrid)\n",
    "<Predicate> ::= <name> \"(\" <Terms> \")\"\n",
    "\n",
    "// Example: CanMake(Luke, ScrambledEggs, Muffins)\n",
    "<Terms> ::= <Term> | <Term> \",\" <Terms>\n",
    "\n",
    "// Example: x or RealMadrid\n",
    "<Term> ::= <Variable> | <Constant>\n",
    "\n",
    "// Example: RealMadrid\n",
    "<Constant> ::= <name>\n",
    "\n",
    "// Example: -OlympicGoldMedalWinner(Amy)\n",
    "<Not> ::= \"-\" <Expression>\n",
    "\n",
    "// Example: LaLigaSoccerTeam(RealMadrid) & LaLigaSoccerTeam(Barcelona)\n",
    "<And> ::= <Expression> \"&\" <Expression>\n",
    "\n",
    "// Example: SpendOn(Amy, MostOfTheirTime, Sports) | OlympicGoldMedalWinner(Amy)\n",
    "<Or> ::= <Expression> \"|\" <Expression>\n",
    "\n",
    "// Example: -NobelPhysicsLaureate(Amy) -> -OlympicGoldMedalWinner(Amy)\n",
    "<Implication> ::= <Expression> \"->\" <Expression>\n",
    "\n",
    "// Example: -OlympicGoldMedalWinner(Amy) -> NobelPhysicsLaureate(Amy)\n",
    "<Biconditional> ::= <Expression> \"<->\" <Expression>\n",
    "\n",
    "// Example: (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & MorePoints(x, y) -> RankHigherThan(x, y))\n",
    "<ParenthesizedExpression> ::= \"(\" <Expression> \")\"\n",
    "\n",
    "// Example: RealMadrid\n",
    "<name> ::= <letter> <letters>\n",
    "\n",
    "// Example: R\n",
    "<letter> ::= \"a\" | \"b\" | \"c\" | ... | \"z\" | \"A\" | \"B\" | \"C\" | ... | \"Z\"\n",
    "\n",
    "// Example: RealMadrid\n",
    "<letters> ::= <letter> | <letter> <letters>\n",
    "\n",
    "```\n",
    "\n",
    "Here are four examples of premises and corresponding FOL.\n",
    "\n",
    "TEXT: A La Liga soccer team ranks higher than another La Liga soccer team if it receives more points.\n",
    "FOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & MorePoints(x, y) -> RankHigherThan(x, y))\n",
    "\n",
    "TEXT: If there are two La Liga soccer teams and neither has more points than the other, then the team which receives more points from the games between the two teams ranks higher.\n",
    "FOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & -MorePoints(x, y) & -MorePoints(y, x) & MorePointsInGameBetween(x, y) ->  RankHigherThan(x, y))\n",
    "\n",
    "TEXT: Real Madrid and Barcelona are both La Liga soccer teams.\n",
    "FOL: LaLigaSoccerTeam(RealMadrid) & LaLigaSoccerTeam(Barcelona)\n",
    "\n",
    "TEXT: Real Madrid received more points than Barcelona.\n",
    "FOL: MorePoints(RealMadrid, Barcelona)\n",
    "\n",
    "TEXT: Neither Real Madrid nor Barcelona received more points from the games between them.\n",
    "FOL: -MorePointsInGameBetween(RealMadrid, Barcelona) & -MorePointsInGameBetween(Barcelona, RealMadrid)\n",
    "\n",
    "TEXT: Real Madrid ranks higher than Barcelona.\n",
    "FOL: RankHigherThan(RealMadrid, Barcelona)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: All professional athletes spend most of their time on sports.\n",
    "FOL: all x. (ProfessionalAthlete(x) -> SpendOn(x, MostOfTheirTime, Sports))\n",
    "\n",
    "TEXT: All Olympic gold medal winners are professional athletes.\n",
    "FOL: all x. (OlympicGoldMedalWinner(x) -> ProfessionalAthlete(x))\n",
    "\n",
    "TEXT: No full-time scientists spend the majority of their time on sports.\n",
    "FOL: all x. (FullTimeScientist(x) -> -SpendOn(x, MostOfTheirTime, Sports))\n",
    "\n",
    "TEXT: All Nobel physics laureates are full-time scientists.\n",
    "FOL: all x. (NobelPhysicsLaureate(x) -> FullTimeScientist(x))\n",
    "\n",
    "TEXT: Amy spends the most time on sports, or Amy is an Olympic gold medal winner.\n",
    "FOL: SpendOn(Amy, MostOfTheirTime, Sports) | OlympicGoldMedalWinner(Amy)\n",
    "\n",
    "TEXT: If Amy is not a Nobel physics laureate, then Amy is not an Olympic gold medal winner.\n",
    "FOL: -NobelPhysicsLaureate(Amy) -> -OlympicGoldMedalWinner(Amy)\n",
    "\n",
    "TEXT: If Amy is not an Olympic gold medal winner, then Amy is a Nobel physics laureate.\n",
    "FOL: -OlympicGoldMedalWinner(Amy) -> NobelPhysicsLaureate(Amy)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: No songs are visuals.\n",
    "FOL: all x. (Song(x) -> -Visual(x))\n",
    "\n",
    "TEXT: All folk songs are songs.\n",
    "FOL: all x. (FolkSong(x) -> Song(x))\n",
    "\n",
    "TEXT: All videos are visuals.\n",
    "FOL: all x. (Video(x) -> Visual(x))\n",
    "\n",
    "TEXT: All movies are videos.\n",
    "FOL: all x. (Movie(x) -> Video(x))\n",
    "\n",
    "TEXT: All sci-fi movies are movies.\n",
    "FOL: all x. (ScifiMovie(x) -> Movie(x))\n",
    "\n",
    "TEXT: Inception is a sci-fi movie.\n",
    "FOL: ScifiMovie(Inception)\n",
    "\n",
    "TEXT: Mac is neither a folk song nor a sci-fi movie.\n",
    "FOL: -FolkSong(Mac) & -ScifiMovie(Mac)\n",
    "\n",
    "TEXT: Inception is a folk song.\n",
    "FOL: FolkSong(Inception)\n",
    "<EVALUATE>\n",
    "\n",
    "TEXT: Every chef can cook.\n",
    "FOL: all x. (Chef(x) -> Can(x, Cook))\n",
    "\n",
    "TEXT: Some people who aren’t chefs can cook.\n",
    "FOL: exists x. (-Chef(x) & Can(x, Cook))\n",
    "\n",
    "TEXT: People who cook can make scrambled eggs and pasta.\n",
    "FOL: all x. (Can(x, Cook) -> (CanMake(x, ScrambledEggs) & CanMake(x, Pasta)))\n",
    "\n",
    "TEXT: If someone can make cookies and muffins, they are a baker.\n",
    "FOL: all x. (CanMake(x, Cookies) & CanMake(x, Muffins) -> Baker(x))\n",
    "\n",
    "TEXT: Bakers who can also make scrambled eggs can make a good breakfast.\n",
    "FOL: all x. ((Baker(x) & CanMake(x, ScrambledEggs)) -> CanMake(x, GoodBreakfast))\n",
    "\n",
    "TEXT: Luke can make cookies, scrambled eggs, and muffins, but not pasta.\n",
    "FOL: CanMake(Luke, Cookies) & (CanMake(Luke, ScrambledEggs) & CanMake(Luke, Muffins) & -CanMake(Luke, Pasta)\n",
    "\n",
    "TEXT: Luke is a chef.\n",
    "FOL: Chef(Luke)\n",
    "<EVALUATE>\n",
    "\n",
    "\n",
    "Translate the following premises and conclusions to FOL expressions that are parseable by NLTK.\n",
    "Only output the expressions.\n",
    "<PREMISES>\n",
    "\"\"\"\n",
    "\n",
    "theRest = \"\"\"Surprises are either fun or dreadful.\n",
    "All scares are surprises.\n",
    "</PREMISES>\n",
    "<CONCLUSION>\n",
    "All scares are fun.\n",
    "</CONCLUSION>\n",
    "<EVALUATE>\n",
    "\"\"\"\n",
    "\n",
    "def genPrompt(fragment, premises, conclusion):\n",
    "    return fragment + premises + \"\\n</PREMISES>\\n<CONCLUSION>\" + conclusion + \"\\n</CONCLUSION>\\n<EVALUATE>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fdf732e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following is a first-order logic (FOL) problem.\\nThe problem is to determine whether the conclusion follows from the premises.\\nThe premises are given in the form of a set of first-order logic sentences.\\nThe conclusion is given in the form of a single first-order logic sentence.\\nThe task is to translate each of the premises and conclusions into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\\nExpressions should adhere to the format of the Python NLTK package logic module.\\n\\nHere are four examples of premises and corresponding FOL.\\n\\nTEXT: A La Liga soccer team ranks higher than another La Liga soccer team if it receives more points.\\nFOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & MorePoints(x, y) -> RankHigherThan(x, y))\\n\\nTEXT: If there are two La Liga soccer teams and neither has more points than the other, then the team which receives more points from the games between the two teams ranks higher.\\nFOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & -MorePoints(x, y) & -MorePoints(y, x) & MorePointsInGameBetween(x, y) ->  RankHigherThan(x, y))\\n\\nTEXT: Real Madrid and Barcelona are both La Liga soccer teams.\\nFOL: LaLigaSoccerTeam(RealMadrid) & LaLigaSoccerTeam(Barcelona)\\n\\nTEXT: Real Madrid received more points than Barcelona.\\nFOL: MorePoints(RealMadrid, Barcelona)\\n\\nTEXT: Neither Real Madrid nor Barcelona received more points from the games between them.\\nFOL: -MorePointsInGameBetween(RealMadrid, Barcelona) & -MorePointsInGameBetween(Barcelona, RealMadrid)\\n\\nTEXT: Real Madrid ranks higher than Barcelona.\\nFOL: RankHigherThan(RealMadrid, Barcelona)\\n<EVALUATE>\\n\\nTEXT: All professional athletes spend most of their time on sports.\\nFOL: all x. (ProfessionalAthlete(x) -> SpendOn(x, MostOfTheirTime, Sports))\\n\\nTEXT: All Olympic gold medal winners are professional athletes.\\nFOL: all x. (OlympicGoldMedalWinner(x) -> ProfessionalAthlete(x))\\n\\nTEXT: No full-time scientists spend the majority of their time on sports.\\nFOL: all x. (FullTimeScientist(x) -> -SpendOn(x, MostOfTheirTime, Sports))\\n\\nTEXT: All Nobel physics laureates are full-time scientists.\\nFOL: all x. (NobelPhysicsLaureate(x) -> FullTimeScientist(x))\\n\\nTEXT: Amy spends the most time on sports, or Amy is an Olympic gold medal winner.\\nFOL: SpendOn(Amy, MostOfTheirTime, Sports) | OlympicGoldMedalWinner(Amy)\\n\\nTEXT: If Amy is not a Nobel physics laureate, then Amy is not an Olympic gold medal winner.\\nFOL: -NobelPhysicsLaureate(Amy) -> -OlympicGoldMedalWinner(Amy)\\n\\nTEXT: If Amy is not an Olympic gold medal winner, then Amy is a Nobel physics laureate.\\nFOL: -OlympicGoldMedalWinner(Amy) -> NobelPhysicsLaureate(Amy)\\n<EVALUATE>\\n\\nTEXT: No songs are visuals.\\nFOL: all x. (Song(x) -> -Visual(x))\\n\\nTEXT: All folk songs are songs.\\nFOL: all x. (FolkSong(x) -> Song(x))\\n\\nTEXT: All videos are visuals.\\nFOL: all x. (Video(x) -> Visual(x))\\n\\nTEXT: All movies are videos.\\nFOL: all x. (Movie(x) -> Video(x))\\n\\nTEXT: All sci-fi movies are movies.\\nFOL: all x. (ScifiMovie(x) -> Movie(x))\\n\\nTEXT: Inception is a sci-fi movie.\\nFOL: ScifiMovie(Inception)\\n\\nTEXT: Mac is neither a folk song nor a sci-fi movie.\\nFOL: -FolkSong(Mac) & -ScifiMovie(Mac)\\n\\nTEXT: Inception is a folk song.\\nFOL: FolkSong(Inception)\\n<EVALUATE>\\n\\nTEXT: Every chef can cook.\\nFOL: all x. (Chef(x) -> Can(x, Cook))\\n\\nTEXT: Some people who aren’t chefs can cook.\\nFOL: exists x. (-Chef(x) & Can(x, Cook))\\n\\nTEXT: People who cook can make scrambled eggs and pasta.\\nFOL: all x. (Can(x, Cook) -> (CanMake(x, ScrambledEggs) & CanMake(x, Pasta)))\\n\\nTEXT: If someone can make cookies and muffins, they are a baker.\\nFOL: all x. (CanMake(x, Cookies) & CanMake(x, Muffins) -> Baker(x))\\n\\nTEXT: Bakers who can also make scrambled eggs can make a good breakfast.\\nFOL: all x. ((Baker(x) & CanMake(x, ScrambledEggs)) -> CanMake(x, GoodBreakfast))\\n\\nTEXT: Luke can make cookies, scrambled eggs, and muffins, but not pasta.\\nFOL: CanMake(Luke, Cookies) & (CanMake(Luke, ScrambledEggs) & CanMake(Luke, Muffins) & -CanMake(Luke, Pasta)\\n\\nTEXT: Luke is a chef.\\nFOL: Chef(Luke)\\n<EVALUATE>\\n\\nTranslate the following premises and conclusions to FOL expressions that are parseable by NLTK.\\nOnly output the expressions.\\n<PREMISES>\\nPeople in this club who perform in school talent shows often attend and are very engaged with school events.\\nPeople in this club either perform in school talent shows often or are inactive and disinterested community members.\\nPeople in this club who chaperone high school dances are not students who attend the school.\\nAll people in this club who are inactive and disinterested members of their community chaperone high school dances.\\nAll young children and teenagers in this club who wish to further their academic careers and educational opportunities are students who attend the school. \\nBonnie is in this club and she either both attends and is very engaged with school events and is a student who attends the school or is not someone who both attends and is very engaged with school events and is not a student who attends the school.\\n</PREMISES>\\n<CONCLUSION>Bonnie performs in school talent shows often.\\n</CONCLUSION>\\n<EVALUATE>\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull out validation set, 203 rows\n",
    "validation = pd.read_json(\"hf://datasets/yale-nlp/FOLIO/\" + splits[\"validation\"], lines=True)\n",
    "\n",
    "# testing genPrompt\n",
    "genPrompt(normalFragment, validation['premises'][0], validation['conclusion'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61c199fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "# call to NLP model 1: gemini flash 1.5\n",
    "gem_api_key = \"AIzaSyDSGZkYYcm0lKI4BHZuN2mks8G0HBYlA0U\"\n",
    "\n",
    "def call_gem_api(key, prompt):\n",
    "    data = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    command = [\n",
    "        \"curl\",\n",
    "        \"-H\", \"Content-Type: application/json\",\n",
    "        \"-d\", json.dumps(data),\n",
    "        \"-X\", \"POST\", f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={key}\",\n",
    "    ]\n",
    "\n",
    "    result = subprocess.run(command, capture_output=True, text=True) \n",
    "    response_json = json.loads(result.stdout)\n",
    "    return response_json\n",
    "\n",
    "# call to model 2: starcoder\n",
    "star_api_key = \"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\"\n",
    "\n",
    "def call_star_api(key, prompt):\n",
    "    data = {\n",
    "        \"inputs\": prompt, \n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,  \n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    }\n",
    "\n",
    "    command = [\n",
    "        \"curl\",\n",
    "        \"-X\", \"POST\",\n",
    "        \"-H\", f\"Authorization: Bearer {key}\",  \n",
    "        \"-H\", \"Content-Type: application/json\",\n",
    "        \"-d\", json.dumps(data),\n",
    "        \"https://api-inference.huggingface.co/models/bigcode/starcoder\"\n",
    "    ]\n",
    "\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        raise Exception(f\"API call failed: {result.stderr}\")\n",
    "\n",
    "    response_json = json.loads(result.stdout)\n",
    "    return response_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acbe8c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidates': [{'content': {'parts': [{'text': 'all x. (Bird(x) -> CanFly(x))\\nall x. (Penguin(x) -> Bird(x))\\nall x. (Penguin(x) -> -CanFly(x))\\n<PREMISES>\\n'}],\n",
       "    'role': 'model'},\n",
       "   'finishReason': 'STOP',\n",
       "   'avgLogprobs': -0.046473484413296566}],\n",
       " 'usageMetadata': {'promptTokenCount': 1203,\n",
       "  'candidatesTokenCount': 51,\n",
       "  'totalTokenCount': 1254},\n",
       " 'modelVersion': 'gemini-1.5-flash-latest'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = call_gem_api(gem_api_key, normalFragment)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4f2bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-host starcoder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# laod model and tokenizer\n",
    "def load_starcoder_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoderplus\")\n",
    "    #model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder\", ddevice_map=\"balanced_low_0\", torch_dtype=torch.float16)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoderplus\").to(\"cpu\")\n",
    "\n",
    "    \n",
    "    # Set the pad_token_id if it's not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "# generate starcoder response \n",
    "def generate_response(model, tokenizer, prompt, max_tokens=200, temperature=0.5, top_p=0.90):\n",
    "    # tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "    \n",
    "\n",
    "    # generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "    # decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # extract only relevant FOL expressions\n",
    "    lines = generated_text.split(\"\\n\")\n",
    "    result = [line.replace(\"FOL:\", \"\").strip() for line in lines if line.strip().startswith(\"FOL:\")]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ddaeb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 7/7 [09:31<00:00, 81.60s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.10it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/shared/25rj4/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/shared/25rj4/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & MorePoints(x, y) -> RankHigherThan(x, y))',\n",
       " 'all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & -MorePoints(x, y) & -MorePoints(y, x) & MorePointsInGameBetween(x, y) ->  RankHigherThan(x, y))',\n",
       " 'LaLigaSoccerTeam(RealMadrid) & LaLigaSoccerTeam(Barcelona)',\n",
       " 'MorePoints(RealMadrid, Barcelona)',\n",
       " '-MorePointsInGameBetween(RealMadrid, Barcelona) & -MorePointsInGameBetween(Barcelona, RealMadrid)',\n",
       " 'RankHigherThan(RealMadrid, Barcelona)',\n",
       " 'all x. (ProfessionalAthlete(x) -> SpendOn(x, MostOfTheirTime, Sports))',\n",
       " 'all x. (OlympicGoldMedalWinner(x) -> ProfessionalAthlete(x))',\n",
       " 'all x. (FullTimeScientist(x) -> -SpendOn(x, MostOfTheirTime, Sports))',\n",
       " 'all x. (NobelPhysicsLaureate(x) -> FullTimeScientist(x))',\n",
       " 'SpendOn(Amy, MostOfTheirTime, Sports) | OlympicGoldMedalWinner(Amy)',\n",
       " '-NobelPhysicsLaureate(Amy) -> -OlympicGoldMedalWinner(Amy)',\n",
       " '-OlympicGoldMedalWinner(Amy) -> NobelPhysicsLaureate(Amy)',\n",
       " 'all x. (Song(x) -> -Visual(x))',\n",
       " 'all x. (FolkSong(x) -> Song(x))',\n",
       " 'all x. (Video(x) -> Visual(x))',\n",
       " 'all x. (Movie(x) -> Video(x))',\n",
       " 'all x. (ScifiMovie(x) -> Movie(x))',\n",
       " 'ScifiMovie(Inception)',\n",
       " '-FolkSong(Mac) & -ScifiMovie(Mac)',\n",
       " 'FolkSong(Inception)',\n",
       " 'all x. (Chef(x) -> Can(x, Cook))',\n",
       " 'exists x. (-Chef(x) & Can(x, Cook))',\n",
       " 'all x. (Can(x, Cook) -> (CanMake(x, ScrambledEggs) & CanMake(x, Pasta)))',\n",
       " 'all x. (CanMake(x, Cookies) & CanMake(x, Muffins) -> Baker(x))',\n",
       " 'all x. ((Baker(x) & CanMake(x, ScrambledEggs)) -> CanMake(x, GoodBreakfast))',\n",
       " 'CanMake(Luke, Cookies) & (CanMake(Luke, ScrambledEggs) & CanMake(Luke, Muffins) & -CanMake(Luke, Pasta)',\n",
       " 'Chef(Luke)',\n",
       " 'all x. (ProfessionalAthlete(x) -> SpendOn(x, MostOfTheirTime, Sports))',\n",
       " 'all x. (ProfessionalAthlete(x) -> SpendOn(x, MostOfTheirTime, Sports))',\n",
       " 'all x. (ProfessionalAthlete(x) -> SpendOn(x, MostOfTheirTime, Sports))']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = load_starcoder_model()\n",
    "response = generate_response(model, tokenizer, normalFragment)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71818cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Input is too long, try to truncate or use a paramater to handle this: The expanded size of the tensor (1353) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 1353].  Tensor sizes: [1, 514]', 'warnings': ['There was an inference error: Input is too long, try to truncate or use a paramater to handle this: The expanded size of the tensor (1353) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 1353].  Tensor sizes: [1, 514]']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def call_huggingface_api(prompt):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    headers = {\"Authorization\": \"Bearer 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\"}\n",
    "    data = {\n",
    "        \"inputs\": prompt,\n",
    "    }\n",
    "\n",
    "    response = requests.post(API_URL, headers=headers, json=data)\n",
    "    return response.json()\n",
    "    \n",
    "response = call_huggingface_api(normalFragment)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa512a3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The following is a first-order logic (FOL) problem.\\nThe problem is to determine whether the conclusion follows from the premises.\\nThe premises are given in the form of a set of first-order logic sentences.\\nThe conclusion is given in the form of a single first-order logic sentence.\\nThe task is to translate each of the premises and conclusions into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\\nExpressions should adhere to the format of the Python NLTK package logic module.\\n\\nHere are four examples of premises and corresponding FOL.\\n\\nTEXT: A La Liga soccer team ranks higher than another La Liga soccer team if it receives more points.\\nFOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & MorePoints(x, y) -> RankHigherThan(x, y))\\n\\nTEXT: If there are two La Liga soccer teams and neither has more points than the other, then the team which receives more points from the games between the two teams ranks higher.\\nFOL: all x. all y. (LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & -MorePoints(x, y) & -MorePoints(y, x) & MorePointsInGameBetween(x, y) ->  RankHigherThan(x, y))\\n\\nTEXT: Real Madrid and Barcelona are both La Liga soccer teams.\\nFOL: LaLigaSoccerTeam(RealMadrid) & LaLigaSoccerTeam(Barcelona)\\n\\nTEXT: Real Madrid received more points than Barcelona.\\nFOL: MorePoints(RealMadrid, Barcelona)\\n\\nTEXT: Neither Real Madrid nor Barcelona received more points from the games between them.\\nFOL: -MorePointsInGameBetween(RealMadrid, Barcelona) & -MorePointsInGameBetween(Barcelona, RealMadrid)\\n\\nTEXT: Real Madrid ranks higher than Barcelona.\\nFOL: RankHigherThan(RealMadrid, Barcelona)\\n<EVALUATE>\\n\\nTEXT: All professional athletes spend most of their time on sports.\\nFOL: all x. (ProfessionalAthlete(x) -> SpendOn(x, MostOfTheirTime, Sports))\\n\\nTEXT: All Olympic gold medal winners are professional athletes.\\nFOL: all x. (OlympicGoldMedalWinner(x) -> ProfessionalAthlete(x))\\n\\nTEXT: No full-time scientists spend the majority of their time on sports.\\nFOL: all x. (FullTimeScientist(x) -> -SpendOn(x, MostOfTheirTime, Sports))\\n\\nTEXT: All Nobel physics laureates are full-time scientists.\\nFOL: all x. (NobelPhysicsLaureate(x) -> FullTimeScientist(x))\\n\\nTEXT: Amy spends the most time on sports, or Amy is an Olympic gold medal winner.\\nFOL: SpendOn(Amy, MostOfTheirTime, Sports) | OlympicGoldMedalWinner(Amy)\\n\\nTEXT: If Amy is not a Nobel physics laureate, then Amy is not an Olympic gold medal winner.\\nFOL: -NobelPhysicsLaureate(Amy) -> -OlympicGoldMedalWinner(Amy)\\n\\nTEXT: If Amy is not an Olympic gold medal winner, then Amy is a Nobel physics laureate.\\nFOL: -OlympicGoldMedalWinner(Amy) -> NobelPhysicsLaureate(Amy)\\n<EVALUATE>\\n\\nTEXT: No songs are visuals.\\nFOL: all x. (Song(x) -> -Visual(x))\\n\\nTEXT: All folk songs are songs.\\nFOL: all x. (FolkSong(x) -> Song(x))\\n\\nTEXT: All videos are visuals.\\nFOL: all x. (Video(x) -> Visual(x))\\n\\nTEXT: All movies are videos.\\nFOL: all x. (Movie(x) -> Video(x))\\n\\nTEXT: All sci-fi movies are movies.\\nFOL: all x. (ScifiMovie(x) -> Movie(x))\\n\\nTEXT: Inception is a sci-fi movie.\\nFOL: ScifiMovie(Inception)\\n\\nTEXT: Mac is neither a folk song nor a sci-fi movie.\\nFOL: -FolkSong(Mac) & -ScifiMovie(Mac)\\n\\nTEXT: Inception is a folk song.\\nFOL: FolkSong(Inception)\\n<EVALUATE>\\n\\nTEXT: Every chef can cook.\\nFOL: all x. (Chef(x) -> Can(x, Cook))\\n\\nTEXT: Some people who aren’t chefs can cook.\\nFOL: exists x. (-Chef(x) & Can(x, Cook))\\n\\nTEXT: People who cook can make scrambled eggs and pasta.\\nFOL: all x. (Can(x, Cook) -> (CanMake(x, ScrambledEggs) & CanMake(x, Pasta)))\\n\\nTEXT: If someone can make cookies and muffins, they are a baker.\\nFOL: all x. (CanMake(x, Cookies) & CanMake(x, Muffins) -> Baker(x))\\n\\nTEXT: Bakers who can also make scrambled eggs can make a good breakfast.\\nFOL: all x. ((Baker(x) & CanMake(x, ScrambledEggs)) -> CanMake(x, GoodBreakfast))\\n\\nTEXT: Luke can make cookies, scrambled eggs, and muffins, but not pasta.\\nFOL: CanMake(Luke, Cookies) & (CanMake(Luke, ScrambledEggs) & CanMake(Luke, Muffins) & -CanMake(Luke, Pasta)\\n\\nTEXT: Luke is a chef.\\nFOL: Chef(Luke)\\n<EVALUATE>\\n\\nTranslate the following premises and conclusions to FOL expressions that are parseable by NLTK.\\nOnly output the expressions.\\n<PREMISES>\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot(x)\\n\\nTEXT: The sun is hot.\\nFOL: Sun(x) & Hot'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = call_star_api(star_api_key, normalFragment)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9445466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# input which api you want to call, \"gem\" for gemini flash and \"star\" for starcoder\n",
    "def process_dataset(dataset, fragment, rate_limit, which_api):\n",
    "    start = time.time()\n",
    "    lst = []\n",
    "    numParsedExprs = 0\n",
    "    numTotalExprs = 0\n",
    "    for i in range(len(dataset)):\n",
    "        # rate limit\n",
    "        #if (i + 1) % rate_limit == 0:\n",
    "        #    since = time.time() - start\n",
    "        #    if since < 60:\n",
    "        #       time.sleep(60 - since)\n",
    "        #    start = time.time()\n",
    "            \n",
    "        premises = dataset['premises'][i]\n",
    "        conclusion = dataset['conclusion'][i]\n",
    "         \n",
    "        # call api\n",
    "        if which_api == \"gem\":\n",
    "            r = call_gem_api(gem_api_key, genPrompt(fragment, premises, conclusion))\n",
    "            print(r)\n",
    "            print(i, \"called API\")\n",
    "            r = r['candidates'][0]['content']['parts'][0]['text'].split('\\n')\n",
    "            r = [l for l in r if not l.startswith(\"`\")]\n",
    "        elif which_api == \"star\":\n",
    "            #r = call_star_api(star_api_key, genPrompt(fragment, premises, conclusion))\n",
    "            #print(r)\n",
    "            #print(i, \"called API\")\n",
    "            #generated_text = r[0]['generated_text']\n",
    "            #r = [segment.strip() for segment in generated_text.split('FOL:') if segment.strip()]\n",
    "            #r = [line for line in r if not line.startswith(\"TEXT:\")]\n",
    "            #print(r)\n",
    "            r = generate_response(model, tokenizer, genPrompt(fragment, premises, conclusion))\n",
    "        \n",
    "        wasException = False\n",
    "        \n",
    "        # parse as nltk Expression\n",
    "        for l in r:\n",
    "            numTotalExprs+=1\n",
    "            try:\n",
    "                expr = Expression.fromstring(l)\n",
    "                numParsedExprs+=1\n",
    "                print(i, expr)\n",
    "            except Exception as e:\n",
    "                wasException = True\n",
    "                continue\n",
    "\n",
    "        if not wasException:\n",
    "            lst.append(i)\n",
    "    print(\"number of parsed exprs: \", numParsedExprs)\n",
    "    print(\"number of total exprs: \", numTotalExprs)\n",
    "    return lst\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2769ab3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'candidates': [{'content': {'parts': [{'text': 'all x. (InClub(x) & PerformInTalentShowOften(x) -> (AttendSchoolEvents(x) & VeryEngaged(x)))\\nall x. (InClub(x) -> (PerformInTalentShowOften(x) | (Inactive(x) & Disinterested(x))))\\nall x. (InClub(x) & ChaperoneHighSchoolDances(x) -> -SchoolStudent(x))\\nall x. (InClub(x) & Inactive(x) & Disinterested(x) -> ChaperoneHighSchoolDances(x))\\nall x. (InClub(x) & YoungChild(x) & WishFurtherAcademicCareer(x) -> SchoolStudent(x))\\nall x. (InClub(x) & Teenager(x) & WishFurtherAcademicCareer(x) -> SchoolStudent(x))\\n(InClub(Bonnie) & ((AttendSchoolEvents(Bonnie) & VeryEngaged(Bonnie) & SchoolStudent(Bonnie)) | (-AttendSchoolEvents(Bonnie) & -VeryEngaged(Bonnie) & -SchoolStudent(Bonnie))))\\nPerformInTalentShowOften(Bonnie)\\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'avgLogprobs': -0.017592947657515363}], 'usageMetadata': {'promptTokenCount': 1388, 'candidatesTokenCount': 246, 'totalTokenCount': 1634}, 'modelVersion': 'gemini-1.5-flash-latest'}\n",
      "0 called API\n",
      "0 all x.((InClub(x) & PerformInTalentShowOften(x)) -> (AttendSchoolEvents(x) & VeryEngaged(x)))\n",
      "0 all x.(InClub(x) -> (PerformInTalentShowOften(x) | (Inactive(x) & Disinterested(x))))\n",
      "0 all x.((InClub(x) & ChaperoneHighSchoolDances(x)) -> -SchoolStudent(x))\n",
      "0 all x.((InClub(x) & Inactive(x) & Disinterested(x)) -> ChaperoneHighSchoolDances(x))\n",
      "0 all x.((InClub(x) & YoungChild(x) & WishFurtherAcademicCareer(x)) -> SchoolStudent(x))\n",
      "0 all x.((InClub(x) & Teenager(x) & WishFurtherAcademicCareer(x)) -> SchoolStudent(x))\n",
      "0 (InClub(Bonnie) & ((AttendSchoolEvents(Bonnie) & VeryEngaged(Bonnie) & SchoolStudent(Bonnie)) | (-AttendSchoolEvents(Bonnie) & -VeryEngaged(Bonnie) & -SchoolStudent(Bonnie))))\n",
      "0 PerformInTalentShowOften(Bonnie)\n",
      "{'candidates': [{'content': {'parts': [{'text': 'all x. ((PersonInThisClub(x) & PerformInSchoolTalentShowsOften(x)) -> (AttendSchoolEvents(x) & VeryEngagedWithSchoolEvents(x)))\\nall x. (PersonInThisClub(x) -> (PerformInSchoolTalentShowsOften(x) | (InactiveAndDisinterestedCommunityMember(x))))\\nall x. ((PersonInThisClub(x) & ChaperoneHighSchoolDances(x)) -> -StudentWhoAttendsSchool(x))\\nall x. ((PersonInThisClub(x) & InactiveAndDisinterestedCommunityMember(x)) -> ChaperoneHighSchoolDances(x))\\nall x. ((PersonInThisClub(x) & YoungChildOrTeenager(x) & WishToFurtherAcademicCareer(x)) -> StudentWhoAttendsSchool(x))\\n(PersonInThisClub(Bonnie) & ((AttendSchoolEvents(Bonnie) & VeryEngagedWithSchoolEvents(Bonnie) & StudentWhoAttendsSchool(Bonnie)) | (-AttendSchoolEvents(Bonnie) & -VeryEngagedWithSchoolEvents(Bonnie) & -StudentWhoAttendsSchool(Bonnie))))\\n((PersonInThisClub(Bonnie) & (YoungChildOrTeenager(Bonnie) & WishToFurtherAcademicCareer(Bonnie) & ChaperoneHighSchoolDances(Bonnie)) | (PersonInThisClub(Bonnie) & -YoungChildOrTeenager(Bonnie) & -WishToFurtherAcademicCareer(Bonnie))) -> (StudentWhoAttendsSchool(Bonnie) | InactiveAndDisinterestedCommunityMember(Bonnie)))\\n\\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'avgLogprobs': -0.015130348952419787}], 'usageMetadata': {'promptTokenCount': 1446, 'candidatesTokenCount': 332, 'totalTokenCount': 1778}, 'modelVersion': 'gemini-1.5-flash-latest'}\n",
      "1 called API\n",
      "1 all x.((PersonInThisClub(x) & PerformInSchoolTalentShowsOften(x)) -> (AttendSchoolEvents(x) & VeryEngagedWithSchoolEvents(x)))\n",
      "1 all x.(PersonInThisClub(x) -> (PerformInSchoolTalentShowsOften(x) | InactiveAndDisinterestedCommunityMember(x)))\n",
      "1 all x.((PersonInThisClub(x) & ChaperoneHighSchoolDances(x)) -> -StudentWhoAttendsSchool(x))\n",
      "1 all x.((PersonInThisClub(x) & InactiveAndDisinterestedCommunityMember(x)) -> ChaperoneHighSchoolDances(x))\n",
      "1 all x.((PersonInThisClub(x) & YoungChildOrTeenager(x) & WishToFurtherAcademicCareer(x)) -> StudentWhoAttendsSchool(x))\n",
      "1 (PersonInThisClub(Bonnie) & ((AttendSchoolEvents(Bonnie) & VeryEngagedWithSchoolEvents(Bonnie) & StudentWhoAttendsSchool(Bonnie)) | (-AttendSchoolEvents(Bonnie) & -VeryEngagedWithSchoolEvents(Bonnie) & -StudentWhoAttendsSchool(Bonnie))))\n",
      "1 (((PersonInThisClub(Bonnie) & YoungChildOrTeenager(Bonnie) & WishToFurtherAcademicCareer(Bonnie) & ChaperoneHighSchoolDances(Bonnie)) | (PersonInThisClub(Bonnie) & -YoungChildOrTeenager(Bonnie) & -WishToFurtherAcademicCareer(Bonnie))) -> (StudentWhoAttendsSchool(Bonnie) | InactiveAndDisinterestedCommunityMember(Bonnie)))\n",
      "{'candidates': [{'content': {'parts': [{'text': 'all x. ((InClub(x) & PerformInTalentShowOften(x)) -> (AttendSchoolEvents(x) & VeryEngaged(x)))\\nall x. (InClub(x) -> (PerformInTalentShowOften(x) | (Inactive(x) & Disinterested(x))))\\nall x. ((InClub(x) & ChaperoneHighSchoolDances(x)) -> -SchoolStudent(x))\\nall x. ((InClub(x) & Inactive(x) & Disinterested(x)) -> ChaperoneHighSchoolDances(x))\\nall x. ((InClub(x) & YoungChildTeenager(x) & WishFurtherAcademicCareer(x)) -> SchoolStudent(x))\\n(InClub(Bonnie) & ((AttendSchoolEvents(Bonnie) & VeryEngaged(Bonnie) & SchoolStudent(Bonnie)) | (-AttendSchoolEvents(Bonnie) & -VeryEngaged(Bonnie) & -SchoolStudent(Bonnie))))\\n(InClub(Bonnie) & (ChaperoneHighSchoolDances(Bonnie) | (-ChaperoneHighSchoolDances(Bonnie) & PerformInTalentShowOften(Bonnie)))) -> (YoungChildTeenager(Bonnie) & WishFurtherAcademicCareer(Bonnie) & Inactive(Bonnie) & Disinterested(Bonnie))\\n\\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'avgLogprobs': -0.024611077153725746}], 'usageMetadata': {'promptTokenCount': 1432, 'candidatesTokenCount': 277, 'totalTokenCount': 1709}, 'modelVersion': 'gemini-1.5-flash-latest'}\n",
      "2 called API\n",
      "2 all x.((InClub(x) & PerformInTalentShowOften(x)) -> (AttendSchoolEvents(x) & VeryEngaged(x)))\n",
      "2 all x.(InClub(x) -> (PerformInTalentShowOften(x) | (Inactive(x) & Disinterested(x))))\n",
      "2 all x.((InClub(x) & ChaperoneHighSchoolDances(x)) -> -SchoolStudent(x))\n",
      "2 all x.((InClub(x) & Inactive(x) & Disinterested(x)) -> ChaperoneHighSchoolDances(x))\n",
      "2 all x.((InClub(x) & YoungChildTeenager(x) & WishFurtherAcademicCareer(x)) -> SchoolStudent(x))\n",
      "2 (InClub(Bonnie) & ((AttendSchoolEvents(Bonnie) & VeryEngaged(Bonnie) & SchoolStudent(Bonnie)) | (-AttendSchoolEvents(Bonnie) & -VeryEngaged(Bonnie) & -SchoolStudent(Bonnie))))\n",
      "2 ((InClub(Bonnie) & (ChaperoneHighSchoolDances(Bonnie) | (-ChaperoneHighSchoolDances(Bonnie) & PerformInTalentShowOften(Bonnie)))) -> (YoungChildTeenager(Bonnie) & WishFurtherAcademicCareer(Bonnie) & Inactive(Bonnie) & Disinterested(Bonnie)))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# gemini flash output\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# baseline prompt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m gemNormalLst \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalFragment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mlen\u001b[39m(gemNormalLst)\n",
      "Cell \u001b[0;32mIn[30], line 22\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[0;34m(dataset, fragment, rate_limit, which_api)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# call api\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which_api \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgem\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mcall_gem_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgem_api_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenPrompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfragment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpremises\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconclusion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(r)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalled API\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 27\u001b[0m, in \u001b[0;36mcall_gem_api\u001b[0;34m(key, prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     10\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     ]\n\u001b[1;32m     18\u001b[0m }\n\u001b[1;32m     20\u001b[0m command \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-H\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type: application/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-d\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(data),\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-X\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m ]\n\u001b[0;32m---> 27\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     28\u001b[0m response_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(result\u001b[38;5;241m.\u001b[39mstdout)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response_json\n",
      "File \u001b[0;32m~/.conda/envs/cs375final/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/.conda/envs/cs375final/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/cs375final/lib/python3.11/subprocess.py:2115\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2109\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2110\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2115\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cs375final/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# gemini flash output\n",
    "# baseline prompt\n",
    "gemNormalLst = process_dataset(validation, normalFragment, 15, \"gem\")\n",
    "len(gemNormalLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eae0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini flash output\n",
    "# bnf prompt\n",
    "gemBnfList = process_dataset(validation, bnfFragment, 15, \"gem\")\n",
    "len(gemBnfList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77529e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 all x y.((LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & MorePoints(x,y)) -> RankHigherThan(x,y))\n",
      "0 all x y.((LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & -MorePoints(x,y) & -MorePoints(y,x) & MorePointsInGameBetween(x,y)) -> RankHigherThan(x,y))\n",
      "0 (LaLigaSoccerTeam(RealMadrid) & LaLigaSoccerTeam(Barcelona))\n",
      "0 MorePoints(RealMadrid,Barcelona)\n",
      "0 (-MorePointsInGameBetween(RealMadrid,Barcelona) & -MorePointsInGameBetween(Barcelona,RealMadrid))\n",
      "0 RankHigherThan(RealMadrid,Barcelona)\n",
      "0 all x.(ProfessionalAthlete(x) -> SpendOn(x,MostOfTheirTime,Sports))\n",
      "0 all x.(OlympicGoldMedalWinner(x) -> ProfessionalAthlete(x))\n",
      "0 all x.(FullTimeScientist(x) -> -SpendOn(x,MostOfTheirTime,Sports))\n",
      "0 all x.(NobelPhysicsLaureate(x) -> FullTimeScientist(x))\n",
      "0 (SpendOn(Amy,MostOfTheirTime,Sports) | OlympicGoldMedalWinner(Amy))\n",
      "0 (-NobelPhysicsLaureate(Amy) -> -OlympicGoldMedalWinner(Amy))\n",
      "0 (-OlympicGoldMedalWinner(Amy) -> NobelPhysicsLaureate(Amy))\n",
      "0 all x.(Song(x) -> -Visual(x))\n",
      "0 all x.(FolkSong(x) -> Song(x))\n",
      "0 all x.(Video(x) -> Visual(x))\n",
      "0 all x.(Movie(x) -> Video(x))\n",
      "0 all x.(ScifiMovie(x) -> Movie(x))\n",
      "0 ScifiMovie(Inception)\n",
      "0 (-FolkSong(Mac) & -ScifiMovie(Mac))\n",
      "0 FolkSong(Inception)\n",
      "0 all x.(Chef(x) -> Can(x,Cook))\n",
      "0 exists x.(-Chef(x) & Can(x,Cook))\n",
      "0 all x.(Can(x,Cook) -> (CanMake(x,ScrambledEggs) & CanMake(x,Pasta)))\n",
      "0 all x.((CanMake(x,Cookies) & CanMake(x,Muffins)) -> Baker(x))\n",
      "0 all x.((Baker(x) & CanMake(x,ScrambledEggs)) -> CanMake(x,GoodBreakfast))\n",
      "0 Chef(Luke)\n",
      "1 all x y.((LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & MorePoints(x,y)) -> RankHigherThan(x,y))\n",
      "1 all x y.((LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & -MorePoints(x,y) & -MorePoints(y,x) & MorePointsInGameBetween(x,y)) -> RankHigherThan(x,y))\n",
      "1 (LaLigaSoccerTeam(RealMadrid) & LaLigaSoccerTeam(Barcelona))\n",
      "1 MorePoints(RealMadrid,Barcelona)\n",
      "1 (-MorePointsInGameBetween(RealMadrid,Barcelona) & -MorePointsInGameBetween(Barcelona,RealMadrid))\n",
      "1 RankHigherThan(RealMadrid,Barcelona)\n",
      "1 all x.(ProfessionalAthlete(x) -> SpendOn(x,MostOfTheirTime,Sports))\n",
      "1 all x.(OlympicGoldMedalWinner(x) -> ProfessionalAthlete(x))\n",
      "1 all x.(FullTimeScientist(x) -> -SpendOn(x,MostOfTheirTime,Sports))\n",
      "1 all x.(NobelPhysicsLaureate(x) -> FullTimeScientist(x))\n",
      "1 (SpendOn(Amy,MostOfTheirTime,Sports) | OlympicGoldMedalWinner(Amy))\n",
      "1 (-NobelPhysicsLaureate(Amy) -> -OlympicGoldMedalWinner(Amy))\n",
      "1 (-OlympicGoldMedalWinner(Amy) -> NobelPhysicsLaureate(Amy))\n",
      "1 all x.(Song(x) -> -Visual(x))\n",
      "1 all x.(FolkSong(x) -> Song(x))\n",
      "1 all x.(Video(x) -> Visual(x))\n",
      "1 all x.(Movie(x) -> Video(x))\n",
      "1 all x.(ScifiMovie(x) -> Movie(x))\n",
      "1 ScifiMovie(Inception)\n",
      "1 (-FolkSong(Mac) & -ScifiMovie(Mac))\n",
      "1 FolkSong(Inception)\n",
      "1 all x.(Chef(x) -> Can(x,Cook))\n",
      "1 exists x.(-Chef(x) & Can(x,Cook))\n",
      "1 all x.(Can(x,Cook) -> (CanMake(x,ScrambledEggs) & CanMake(x,Pasta)))\n",
      "1 all x.((CanMake(x,Cookies) & CanMake(x,Muffins)) -> Baker(x))\n",
      "1 all x.((Baker(x) & CanMake(x,ScrambledEggs)) -> CanMake(x,GoodBreakfast))\n",
      "1 Chef(Luke)\n",
      "2 all x y.((LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & MorePoints(x,y)) -> RankHigherThan(x,y))\n",
      "2 all x y.((LaLigaSoccerTeam(x) & LaLigaSoccerTeam(y) & -MorePoints(x,y) & -MorePoints(y,x) & MorePointsInGameBetween(x,y)) -> RankHigherThan(x,y))\n",
      "2 (LaLigaSoccerTeam(RealMadrid) & LaLigaSoccerTeam(Barcelona))\n",
      "2 MorePoints(RealMadrid,Barcelona)\n",
      "2 (-MorePointsInGameBetween(RealMadrid,Barcelona) & -MorePointsInGameBetween(Barcelona,RealMadrid))\n",
      "2 RankHigherThan(RealMadrid,Barcelona)\n",
      "2 all x.(ProfessionalAthlete(x) -> SpendOn(x,MostOfTheirTime,Sports))\n",
      "2 all x.(OlympicGoldMedalWinner(x) -> ProfessionalAthlete(x))\n",
      "2 all x.(FullTimeScientist(x) -> -SpendOn(x,MostOfTheirTime,Sports))\n",
      "2 all x.(NobelPhysicsLaureate(x) -> FullTimeScientist(x))\n",
      "2 (SpendOn(Amy,MostOfTheirTime,Sports) | OlympicGoldMedalWinner(Amy))\n",
      "2 (-NobelPhysicsLaureate(Amy) -> -OlympicGoldMedalWinner(Amy))\n",
      "2 (-OlympicGoldMedalWinner(Amy) -> NobelPhysicsLaureate(Amy))\n",
      "2 all x.(Song(x) -> -Visual(x))\n",
      "2 all x.(FolkSong(x) -> Song(x))\n",
      "2 all x.(Video(x) -> Visual(x))\n",
      "2 all x.(Movie(x) -> Video(x))\n",
      "2 all x.(ScifiMovie(x) -> Movie(x))\n",
      "2 ScifiMovie(Inception)\n",
      "2 (-FolkSong(Mac) & -ScifiMovie(Mac))\n",
      "2 FolkSong(Inception)\n",
      "2 all x.(Chef(x) -> Can(x,Cook))\n",
      "2 exists x.(-Chef(x) & Can(x,Cook))\n",
      "2 all x.(Can(x,Cook) -> (CanMake(x,ScrambledEggs) & CanMake(x,Pasta)))\n",
      "2 all x.((CanMake(x,Cookies) & CanMake(x,Muffins)) -> Baker(x))\n",
      "2 all x.((Baker(x) & CanMake(x,ScrambledEggs)) -> CanMake(x,GoodBreakfast))\n",
      "2 Chef(Luke)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# starcoder output\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# baseline prompt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m starNormalLst \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalFragment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mlen\u001b[39m(starNormalLst)\n",
      "Cell \u001b[0;32mIn[28], line 35\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[0;34m(dataset, fragment, rate_limit, which_api)\u001b[0m\n\u001b[1;32m     26\u001b[0m     r \u001b[38;5;241m=\u001b[39m [l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m l\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m which_api \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstar\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#r = call_star_api(star_api_key, genPrompt(fragment, premises, conclusion))\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#print(r)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#r = [line for line in r if not line.startswith(\"TEXT:\")]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m#print(r)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenPrompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfragment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpremises\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconclusion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m wasException \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# parse as nltk Expression\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 27\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(model, tokenizer, prompt, max_tokens, temperature, top_p)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# generate text\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# decode the generated text\u001b[39;00m\n\u001b[1;32m     38\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3258\u001b[0m     outputs,\n\u001b[1;32m   3259\u001b[0m     model_kwargs,\n\u001b[1;32m   3260\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3261\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:1161\u001b[0m, in \u001b[0;36mGPTBigCodeForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03mlabels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1161\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:994\u001b[0m, in \u001b[0;36mGPTBigCodeModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    982\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    983\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    984\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    991\u001b[0m         output_attentions,\n\u001b[1;32m    992\u001b[0m     )\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 994\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:609\u001b[0m, in \u001b[0;36mGPTBigCodeBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    608\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 609\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    618\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:531\u001b[0m, in \u001b[0;36mGPTBigCodeSdpaAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_query:\n\u001b[1;32m    530\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(hidden_states\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 531\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(attn_output)\n\u001b[1;32m    534\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (attn_output, present)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# starcoder output\n",
    "# baseline prompt\n",
    "starNormalLst = process_dataset(validation, normalFragment, 15, \"star\")\n",
    "len(starNormalLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81686796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starcoder output\n",
    "# bnf prompt\n",
    "starBnfList = process_dataset(validation, bnfFragment, 15, \"star\")\n",
    "len(starBnfList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27117b66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "jsonstr = json.dumps(lst)\n",
    "\n",
    "with open('normal.json', 'w') as file: \n",
    "    file.write(jsonstr)\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Create a link to the file\n",
    "display(FileLink('normal.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70146f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = res['candidates'][0]['content']['parts'][0]['text'].split('\\n')\n",
    "#lines = [reformat_fol(l) for l in lines]\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a098f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Expression.fromstring(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# santacoder attempt\n",
    "# # prompt the model\n",
    "\n",
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bigcode/santacoder\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"bigcode/santacoder\", trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # using normal prompt\n",
    "# inputs_n = tokenizer(normal, return_tensors=\"pt\",truncation=True)\n",
    "# outputs_n = model.generate(**inputs_n, max_new_tokens=200, do_sample=True, temperature=0.7) \n",
    "\n",
    "# output_final_n = tokenizer.decode(outputs_n[0], skip_special_tokens=True)\n",
    "\n",
    "# print(output_final_n)\n",
    "\n",
    "\n",
    "# # using bnf prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs375",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
